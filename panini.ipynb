{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models import TfidfModel\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'ignore', 'over': 'warn', 'under': 'ignore', 'invalid': 'ignore'}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.patheffects as PathEffects\n",
    "import matplotlib.pyplot as pyplot\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_palette('muted')\n",
    "sns.set_context(\"notebook\", font_scale=1.5,\n",
    "                rc={\"lines.linewidth\": 2.5})\n",
    "np.seterr(divide='ignore', invalid='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "        result = []\n",
    "        text=\" \".join(text.split())\n",
    "        for token in simple_preprocess(text):\n",
    "            if token not in STOPWORDS and len(token) > 3:\n",
    "                result.append((token))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Documents:\n",
    "    \n",
    "    def  __init__(self,PDseries,save=False):\n",
    "        self.session_id=self.randomString()\n",
    "        self.PDseries=PDseries\n",
    "        self.size=PDseries.shape[0]\n",
    "        self.preprocessed_text=PDseries.map(preprocess)\n",
    "        self.vocabulary_size=0\n",
    "        self.dictionary=None\n",
    "        self.create_dictionary()\n",
    "        self.flag_creation_signal()\n",
    "        self.word2vec=None\n",
    "        self.word2vec_vector_size=None\n",
    "        self.tfidf=None\n",
    "        self.tfidf_vector_matrix=None\n",
    "        self.doc2vec=None\n",
    "        self.doc2vec_vector_matrix=None\n",
    "        if save:\n",
    "            self.pickle_files()\n",
    "        \n",
    "        \n",
    "    def randomString(self,stringLength=8):\n",
    "        lettersAndDigits = string.ascii_letters + string.digits\n",
    "        return ''.join(random.choice(lettersAndDigits) for i in range(stringLength))\n",
    "    \n",
    "    def preprocess(text):\n",
    "        result = []\n",
    "        text=\" \".join(self.text.split())\n",
    "        for token in simple_preprocess(text):\n",
    "            if token not in STOPWORDS and len(token) > 3:\n",
    "                result.append((token))\n",
    "        return result\n",
    "    \n",
    "    def create_dictionary(self):\n",
    "        self.dictionary = corpora.Dictionary(self.preprocessed_text)\n",
    "        self.dictionary.filter_extremes(no_below=100,no_above=0.9, keep_n=100000)\n",
    "        self.vocabulary_size=len(self.dictionary)\n",
    "        return None\n",
    "    \n",
    "    def vectorize_by_tfidf(self):\n",
    "\n",
    "        bow_corpus = [self.dictionary.doc2bow(doc) for doc in self.preprocessed_text]\n",
    "        self.tfidf = TfidfModel(bow_corpus) \n",
    "        self.tfidf_vector_matrix=self.tfidf[bow_corpus]\n",
    "        \n",
    "        print('Tfidf complete')\n",
    "        index = similarities.MatrixSimilarity(self.tfidf_vector_matrix)\n",
    "        similarity_matrix = index[self.tfidf_vector_matrix]\n",
    "        print('Similarity matrix computed')\n",
    "\n",
    "        return similarity_matrix\n",
    "    \n",
    "    def vectorize_by_doc2vec(self,vector_size=10,window=2):\n",
    "    \n",
    "        documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(self.preprocessed_text.values)]\n",
    "        self.doc2vec = Doc2Vec(documents, vector_size=vector_size, window=window, min_count=1, workers=4)\n",
    "        self.doc2vec_vector_matrix=self.doc2vec.docvecs.vectors_docs\n",
    "        \n",
    "        print('doc2vec complete')\n",
    "        similarity_matrix = pairwise_distances(self.doc2vec_vector_matrix, self.doc2vec_vector_matrix, metric='cosine', n_jobs=-1)\n",
    "        print('Similarity matrix computed')\n",
    "        \n",
    "        return similarity_matrix\n",
    "    \n",
    "    def flag_creation_signal(self):\n",
    "        print(\"Session {} created for document size {} with processed vocabulary of {}\".format(self.session_id,self.size,self.vocabulary_size))\n",
    "        return None\n",
    "    \n",
    "    def run_word2vec(self, vector_size=10, window=5, save=True):\n",
    "        self.word2vec_vector_size=vector_size\n",
    "        self.word2vec = gensim.models.Word2Vec(self.preprocessed_text.values, size=vector_size, window=window, min_count=2, workers=10)\n",
    "        self.word2vec.train(self.preprocessed_text.values,total_examples=self.size,epochs=10)\n",
    "        print('word2vec generated')\n",
    "        if save:\n",
    "            self.pickle_files()\n",
    "        return None\n",
    "    \n",
    "    def pickle_files(self):\n",
    "        \n",
    "        F=open(self.session_id+'_dictionary.sav','wb')\n",
    "        pickle.dump(self.dictionary,F)\n",
    "        F.close()\n",
    "        print(\"Dictionary {} saved\".format(self.session_id))\n",
    "        \n",
    "        F=open(self.session_id+'_'+str(self.size)+'_word2vec.sav','wb')\n",
    "        pickle.dump(self.word2vec,F)\n",
    "        F.close()\n",
    "        print(\"word2vec {} saved\".format(self.session_id))\n",
    "        \n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA_TSNE():\n",
    "    \n",
    "    def __init__(self, similarity_matrix,session_id, if_pca=True, pca_dimensions=20):\n",
    "        self.session_id=session_id\n",
    "        self.pca_dimensions=pca_dimensions\n",
    "        self.similarity_matrix=similarity_matrix\n",
    "        self.size=similarity_matrix.shape[0]\n",
    "        self.pca_result=None\n",
    "        self.cum_pca_variance=None\n",
    "        self.x_axis=None\n",
    "        self.y_axis=None\n",
    "        self.pca_result,self.var=self.run_pca()\n",
    "        self.x_axis,self.y_axis=self.run_tsne()\n",
    "        \n",
    "    def run_pca(self): \n",
    "        pca = PCA(n_components=self.pca_dimensions)\n",
    "        pca_result = pca.fit_transform(self.similarity_matrix)\n",
    "        self.cum_pca_variance=np.sum(pca.explained_variance_ratio_)\n",
    "        print('Cumulative explained variation for {} principal components: {}'.format(self.pca_dimensions,self.cum_pca_variance))\n",
    "        \n",
    "        return pca_result, self.cum_pca_variance\n",
    "        \n",
    "    def run_tsne(self):\n",
    "        \n",
    "        tsne=TSNE(n_components=2,verbose=1,perplexity=40,n_iter=300)\n",
    "        tsne_results=tsne.fit_transform(self.pca_result)\n",
    "        \n",
    "        return tsne_results[:,0],tsne_results[:,1]        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plotter():\n",
    "    \n",
    "    def __init__(self, PCA_TSNE_instance,session_id, s=20):\n",
    "        self.session_id=session_id\n",
    "        self.size=PCA_TSNE_instance.size\n",
    "        self.x_axis=PCA_TSNE_instance.x_axis\n",
    "        self.y_axis=PCA_TSNE_instance.y_axis\n",
    "        self.pixel_size=s\n",
    "        self.cum_pca_variance=PCA_TSNE_instance.cum_pca_variance\n",
    "        self.pca_dimensions=PCA_TSNE_instance.pca_dimensions\n",
    "\n",
    "    \n",
    "    def plotter(self,title=None):\n",
    "        \n",
    "        f = pyplot.figure(figsize=(12, 12))\n",
    "        ax = pyplot.subplot(aspect='equal')\n",
    "\n",
    "        sc = ax.scatter(self.x_axis, self.y_axis, lw=0, s=self.pixel_size)\n",
    "        ax.axis('off')\n",
    "        ax.axis('tight')\n",
    "\n",
    "        if self.cum_pca_variance!=None:\n",
    "            \n",
    "            PCA_text='PCA:{} \\nTotal Variance: {} %'.format(self.pca_dimensions,np.round(self.cum_pca_variance*100,2))\n",
    "            pyplot.annotate(PCA_text, xy=(0,0), xytext=(12, 80), va='top',xycoords='axes fraction', textcoords='offset points')\n",
    "\n",
    "        if title!=None:\n",
    "            pyplot.title(title+\" of \"+str(self.size)+\" Articles\")\n",
    "        \n",
    "        pyplot.savefig(self.session_id+'-'+title+'-'+str(self.size)+'-plot.png')\n",
    "\n",
    "        pyplot.show()\n",
    "        pyplot.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
